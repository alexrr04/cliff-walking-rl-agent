\section{Conclusiones}

A lo largo de este trabajo, se ha realizado un estudio exhaustivo sobre la implementación y el rendimiento de tres algoritmos de aprendizaje por refuerzo (Value Iteration, Direct Estimation y Q-Learning) en el entorno Cliff Walking con estocasticidad. Los experimentos y análisis realizados han permitido obtener una comprensión profunda de las capacidades y limitaciones de cada enfoque en este contexto específico.

El análisis comparativo de los tres algoritmos ha revelado diferencias significativas en su rendimiento. El algoritmo Value Iteration ha demostrado ser el más estable y preciso, alcanzando una tasa de éxito perfecta (100\%) y la mejor recompensa media (-64.292). Esto confirma su capacidad para encontrar la política óptima en entornos con modelo conocido, aunque requiere un tiempo de entrenamiento considerable de aproximadamente 30 segundos. Por su parte, Q-Learning se ha revelado como la alternativa más rápida, con un tiempo de entrenamiento promedio de 12.659 segundos. A pesar de mostrar una mayor varianza en sus resultados, mantiene un rendimiento muy competitivo con una tasa de éxito del 99.74\% y una recompensa media de -69.095, lo que lo hace especialmente atractivo para aplicaciones con restricciones de tiempo. Direct Estimation, si bien alcanza resultados satisfactorios con una tasa de éxito del 99.78\% y una recompensa media de -66.392, requiere el mayor tiempo de computación (41.961 segundos), lo que lo posiciona como la opción menos eficiente para este entorno específico.

La experimentación con los hiperparámetros ha revelado aspectos cruciales para el rendimiento de cada algoritmo. Para Value Iteration, se ha encontrado que un factor de descuento alto ($\gamma \geq 0.9$) es fundamental para obtener políticas óptimas, mientras que el umbral de convergencia afecta principalmente al tiempo de entrenamiento. En el caso de Q-Learning, el factor de descuento y la tasa de aprendizaje han demostrado ser parámetros críticos que influyen significativamente en el rendimiento, observándose que valores moderados de la tasa de aprendizaje ($\alpha = 0.5$) con un decaimiento lento (0.99) producen los mejores resultados. Para Direct Estimation, el número de trayectorias ha demostrado tener un impacto más significativo que el factor de descuento en el rendimiento del algoritmo, siendo 1000 trayectorias el valor óptimo encontrado.

En conclusión, se puede decir que la elección del algoritmo más apropiado dependerá de las preferencias del usuario. Value Iteration es la elección óptima cuando se busca la máxima precisión y estabilidad, y el tiempo de computación no es una limitación crítica. Q-Learning destaca como la mejor opción cuando se requiere un equilibrio entre velocidad y rendimiento, siendo especialmente útil en aplicaciones con restricciones temporales. Direct Estimation, aunque funcional, no ofrece ventajas significativas en este entorno específico que justifiquen su mayor coste computacional. En conjunto, este trabajo demuestra la viabilidad de los tres algoritmos para resolver el problema de Cliff Walking, incluso con la introducción de estocasticidad en el entorno.
