\subsection{Experimentación}

\subsubsection{Experimento factor de descuento \& \(\epsilon\) decay}

\paragraph{Diseño experimental}

% TODO: Usar esta tabla para cada experimento
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|p{4cm}|X|} % Especificar el ancho de las columnas
        \hline % Línea horizontal superior
        \textbf{Observación} & El factor de descuento ($\gamma$) y la tasa de decaimiento ($decay$) de exploración son parámetros críticos en el algoritmo Q-Learning. 
        \\ \hline 
        \textbf{Planteamiento} & Para cada pareja de valores de $\gamma$ y $decay$, se compara la tasa de acierto (llegar al estado final), la recompensa media, número de pasos y tiempo de entrenamiento del algoritmo.
        \\ \hline 
        \textbf{Hipótesis} & Un mayor factor de descuento y una tasa de decaimiento de $\epsilon$ más lenta mejorarán el rendimiento del algoritmo.
        \\ \hline 
        \textbf{Método} & 
        \begin{itemize}
            \item Se fijan 1000 episodios de entrenamiento, tasa de aprendizaje \(\alpha = 0.1\) y \(\epsilon\) inicial de $0.9$.
            \item Se eligen los siguientes valores para \(\gamma\) y $decay$: \(\gamma \in \{0.5, 0.7, 0.9, 0.95, 0.99\}\) y $decay \in \{0.8, 0.9, 0.95, 0.99\}$.
            \item Para cada combinación de \(\gamma\) y $decay$, se ejecuta el algoritmo Q-Learning en el entorno.
            \item Se evalúa la política obtenida probándola con 500 episodios.
            \item Se repite el proceso para cada combinación de \(\gamma\) y $decay$ 20 veces.
        \end{itemize}
        \\ \hline
    \end{tabularx}
    \caption{Q-Learning - Experimento 1 - Factor de descuento \& $\epsilon$ decay}
    \label{tab:diseñoQLEarningExp1}
\end{table}

\paragraph{Resultados}

\subsubsection{Experimento tasa de aprendizaje (\(\alpha\)) \& tasa de exploración (\(\epsilon\))}

\paragraph{Diseño experimental}

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|p{4cm}|X|} % Especificar el ancho de las columnas
        \hline % Línea horizontal superior
        \textbf{Observación} & El rendimiento y óptimalidad de la política encontrada por Q-Learning se ven afectados por los valores de la tasa de aprendizaje $\alpha$ y la tasa de exploración $\epsilon$. 
        \\ \hline 
        \textbf{Planteamiento} & Para cada combinación de \(\alpha\) y \(\epsilon\), se compara la tasa de acierto (llegar al estado final), la recompensa media, número de pasos y tiempo de entrenamiento del algoritmo.
        \\ \hline 
        \textbf{Hipótesis} & Un menor valor de \(\alpha\) y un \(\epsilon\) más lento mejorarán el rendimiento del algoritmo.
        \\ \hline 
        \textbf{Método} & 
        \begin{itemize}
            \item Se fijan 1000 episodios de entrenamiento y los mejores valores para \(\gamma\) y $decay$ del experimento anterior.
            \item Se eligen los siguientes valores para \(\alpha\) y \(\epsilon\): \(\alpha \in \{0.1, 0.2, 0.5, 0.8\}\) y \(\epsilon \in \{0.7, 0.9, 0.95, 0.99\}\).
            \item Para cada combinación de \(\alpha\) y \(\epsilon\), se ejecuta el algoritmo Q-Learning en el entorno.
            \item Se evalúa la política obtenida probándola con 500 episodios.
            \item Se repite el proceso para cada combinación de \(\alpha\) y \(\epsilon\) 20 veces.
        \end{itemize}
        \\ \hline
    \end{tabularx}
    \caption{Q-Learning - Experimento 2 - Tasa de aprendizaje \& tasa de exploración}
    \label{tab:diseñoQLEarningExp2}
\end{table}

\paragraph{Resultados}

\subsubsection{Experimento número de episodios}

\paragraph{Diseño experimental}

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|p{4cm}|X|} % Especificar el ancho de las columnas
        \hline % Línea horizontal superior
        \textbf{Observación} & El número de episodios de entrenamiento es un parámetro crítico en el algoritmo Q-Learning.
        \\ \hline
        \textbf{Planteamiento} & Se compara la tasa de acierto (llegar al estado final), la recompensa media, número de pasos y tiempo de entrenamiento del algoritmo para diferentes números de episodios de entrenamiento.
        \\ \hline
        \textbf{Hipótesis} & Un mayor número de episodios de entrenamiento mejorará el rendimiento del algoritmo.
        \\ \hline
        \textbf{Método} &
        \begin{itemize}
            \item Se fijan los mejores valores para \(\gamma\), $decay$, \(\alpha\) y \(\epsilon\) de los experimentos anteriores.
            \item Se eligen los siguientes \textit{números de episodios de entrenamiento}: \{500, 1000, 5000, 10000\}.
            \item Para cada \textit{número de episodios}, se ejecuta el algoritmo Q-Learning en el entorno.
            \item Se evalúa la política obtenida probándola con 500 episodios.
            \item Se repite el proceso para cada número de episodios 20 veces.
        \end{itemize}
        \\ \hline
    \end{tabularx}
    \caption{Q-Learning - Experimento 3 - Número de episodios}
    \label{tab:diseñoQLEarningExp3}
\end{table}

\paragraph{Resultados}

\subsubsection{Experimento penalización de la acción izquierda}

\paragraph{Diseño experimental}

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|p{4cm}|X|} % Especificar el ancho de las columnas
        \hline % Línea horizontal superior
        \textbf{Observación} & Moverse a la izquierda no es deseable para el agente en ningún momento, ya que no le acerca al objetivo. Por lo tanto, penalizarla con una recompensa menor que las demás acciones puede alterar el comportamiento del agente.
        \\ \hline 
        \textbf{Planteamiento} & Se compara la tasa de acierto (llegar al estado final), la recompensa media, número de pasos y tiempo de entrenamiento del algoritmo para diferentes penalizaciones a la acción ``moverse a la izquierda''.
        \\ \hline
        \textbf{Hipótesis} & Penalizar la acción ``moverse a la izquierda'' mejorará el rendimiento del algoritmo.
        \\ \hline
        \textbf{Método} &
        \begin{itemize}
            \item Se fijan los mejores valores para \(\gamma\), $decay$, \(\alpha\), \(\epsilon\) y \textit{número de episodios de entrenamiento} de los experimentos anteriores.
            \item Se eligen los siguientes valores para la penalización de la acción ``moverse a la izquierda'': \{-1, -2, -10, -50\}.
            \item Para cada penalización, se ejecuta el algoritmo Q-Learning en el entorno.
            \item Se evalúa la política obtenida probándola con 500 episodios.
            \item Se repite el proceso para cada penalización 20 veces.
        \end{itemize}
        \\ \hline
    \end{tabularx}
    \caption{Q-Learning - Experimento 4 - Penalización de la acción ``moverse a la izquierda''}
    \label{tab:diseñoQLEarningExp4}
\end{table}

\paragraph{Resultados}